{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "hw2_final.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo942LMOdlh4",
    "colab_type": "text"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DokFOdD1dJEl",
    "colab_type": "code",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import alexnet\n",
    "from torchvision.models import resnet34\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIDLJuIXK_vh",
    "colab_type": "text"
   },
   "source": [
    "**Set Arguments**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d5PkYfqfK_SA",
    "colab_type": "code",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 101 # 101 + 1: There is an extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 5e-3         # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 15      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = 10       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "\n",
    "LOG_FREQUENCY = 5\n",
    "\n",
    "BASE_FILE_PATH = \"DA_RUN18_LR3e-5_ADAMW_EP15_SS15_G01_ALL_TRANSF\"\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gwii0TBHvzh",
    "colab_type": "text"
   },
   "source": [
    "**Define Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QUDdw4j2H0Mc",
    "colab_type": "code",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "# Define transforms for training phase\n",
    "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
    "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
    "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
    "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
    "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalizes tensor with mean and standard deviation\n",
    "])\n",
    "\"\"\"\n",
    "# DATA AUG 1\n",
    "train_transform = transforms.Compose([\n",
    "                                      transforms.RandomApply([transforms.ColorJitter()], p=0.3),\n",
    "                                      transforms.RandomApply([transforms.Grayscale(3)], p=0.3),\n",
    "                                      transforms.Resize(256),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\"\"\"\n",
    "#DATA AUG 2\n",
    "\"\"\"\n",
    "train_transform = transforms.Compose([\n",
    "                                      transforms.RandomChoice([\n",
    "                                                               transforms.RandomHorizontalFlip(p=.5),\n",
    "                                                               transforms.RandomRotation(45)\n",
    "                                                               ]),\n",
    "                                      transforms.Resize(256),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "# Define transforms for the evaluation phase\n",
    "eval_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))                                    \n",
    "])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qYIHPzYLY7i",
    "colab_type": "text"
   },
   "source": [
    "**Prepare Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QfVq_uDHLbsR",
    "colab_type": "code",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "# Clone github repository with data\n",
    "if not os.path.isdir('./Caltech101'):\n",
    "  !git clone https://github.com/lore-lml/machine-learning2020-hw2.git\n",
    "  !mv 'machine-learning2020-hw2/Caltech101' '.'\n",
    "  !rm -rf 'machine-learning2020-hw2'\n",
    "\n",
    "DATA_DIR = 'Caltech101/101_ObjectCategories'\n",
    "from Caltech101.caltech_dataset import Caltech\n",
    "import numpy as np\n",
    "\n",
    "# Prepare Pytorch train/test Datasets\n",
    "trainVal_dataset = Caltech(DATA_DIR, src='train',  transform=train_transform, eval_transform=eval_transform)\n",
    "test_dataset = Caltech(DATA_DIR, src='test', transform=eval_transform)\n",
    "\n",
    "\"\"\"train_len = int(train_dataset.__len__() / 2)\n",
    "val_len = train_dataset.__len__() - train_len\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_len, val_len])\"\"\"\n",
    "\n",
    "train_dataset, val_dataset = trainVal_dataset.get_train_validation_set()\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"# classes train_set: {len(set(train_dataset.get_labels()))}\")\n",
    "print(f\"# classes val_set: {len(set(val_dataset.get_labels()))}\")\n",
    "print(f\"# classes test_set: {len(set(test_dataset.get_labels()))}\")\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYEDQ7Z21ldN",
    "colab_type": "text"
   },
   "source": [
    "**Prepare Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VriRw8SI1nle",
    "colab_type": "code",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbZ1t5Qs2z4j",
    "colab_type": "text"
   },
   "source": [
    "**Prepare Network**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "exHUjtXa22DN",
    "colab_type": "code",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "def get_alexnet(pretrained=False):\n",
    "  net = alexnet(pretrained=pretrained)\n",
    "  # AlexNet has 1000 output neurons, corresponding to the 1000 ImageNet's classes\n",
    "  # We need 101 outputs for Caltech-101\n",
    "  # nn.Linear in pytorch is a fully connected layer\n",
    "  # The convolutional layer is nn.Conv2d\n",
    "  \n",
    "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
    "  return net\n",
    "\n",
    "def get_resnet(pretrained=False):\n",
    "    net = resnet34(pretrained=pretrained)\n",
    "    net.fc = nn.Linear(net.fc.in_features, NUM_CLASSES)\n",
    "    return net\n",
    "\n",
    "def loss_optmizer_scheduler(model):\n",
    "  \n",
    "  # Define loss function\n",
    "  criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
    "\n",
    "  # Choose parameters to optimize\n",
    "  # To access a different set of parameters, you have to access submodules of AlexNet\n",
    "  # (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
    "  # e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
    "  # e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
    "  parameters_to_optimize = model.parameters() # In this case we optimize over all the parameters of AlexNet\n",
    "\n",
    "  # Define optimizer\n",
    "  # An optimizer updates the weights based on loss\n",
    "  # We use SGD with momentum\n",
    "\n",
    "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "  #optimizer = optim.Adam(parameters_to_optimize, lr=LR,amsgrad=True)\n",
    "  #optimizer = optim.AdamW(parameters_to_optimize, lr=LR,amsgrad=True, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "  # Define scheduler\n",
    "  # A scheduler dynamically changes learning rate\n",
    "  # The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
    "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "  return criterion, optimizer, scheduler"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEyL3H_R4qCf",
    "colab_type": "text"
   },
   "source": [
    "**Prepare Training**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PjYERH4Tzyyz",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def validate(net, val_loader, criterion, device=DEVICE):\n",
    "    net.eval()\n",
    "    cumulative_loss =.0\n",
    "    running_corrects = 0\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)      \n",
    "\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "    return cumulative_loss / len(val_loader), running_corrects / float(len(val_dataset))\n",
    "\n",
    "def test(net, test_loader, device=DEVICE):\n",
    "\n",
    "    net = net.eval()\n",
    "    running_corrects = 0\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        net.eval()\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "   \n",
    "    return running_corrects / float(len(test_dataset))\n",
    "\n",
    "\n",
    "def train_batch(net, train_loader, criterion, optimizer, current_step, device=DEVICE):\n",
    "    net.train()\n",
    "    cumulative_loss =.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        cumulative_loss += loss.item()\n",
    "        \n",
    "        if current_step != 0 and current_step % LOG_FREQUENCY == 0:\n",
    "                print('\\t\\tTrain step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_step += 1\n",
    "\n",
    "    return cumulative_loss / len(train_loader), current_step"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jt-555M9VobA",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train_model(net, criterion, optmizer, scheduler, max_epoch=NUM_EPOCHS, device=DEVICE, file_path=BASE_FILE_PATH):\n",
    "    import time, math\n",
    "    net = net.to(device)\n",
    "    tolerance = 3\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    min_val_loss = -1\n",
    "    current_step = 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(f\"STARTING EPOCH {epoch+1} - LR={scheduler.get_last_lr()}...\")\n",
    "        curr_result = train_batch(net, train_dataloader, criterion, optimizer, current_step, device)\n",
    "        curr_train_loss = curr_result[0]\n",
    "        current_step = curr_result[1]\n",
    "        \n",
    "        train_losses.append(curr_train_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        curr_val_loss, curr_val_accuracy = validate(net, val_dataloader, criterion, device)\n",
    "        val_losses.append(curr_val_loss)\n",
    "        val_accuracies.append(curr_val_accuracy)\n",
    "        \n",
    "        print(f\"\\tRESULT EPOCH {epoch+1}:\")\n",
    "        print(f\"\\t\\tTrain Loss: {curr_train_loss}\")\n",
    "        print(f\"\\t\\tVal Loss: {curr_val_loss} - Val Accuracy: {curr_val_accuracy}\\n\")\n",
    "        \n",
    "        if math.isnan(curr_val_loss):\n",
    "            tolerance -= 1\n",
    "        else:\n",
    "            tolerance = 10\n",
    "        \n",
    "        if tolerance == 0:\n",
    "            print(f\"EARLY STOPPING\\n\")\n",
    "            break\n",
    "        \n",
    "        if min_val_loss == -1 or min_val_loss > curr_val_loss:\n",
    "            min_val_loss = curr_val_loss\n",
    "            torch.save(net, f\"{file_path}_best_model_finetuning.pth\")\n",
    "\n",
    "    net = torch.load(f\"{file_path}_best_model_finetuning.pth\").to(device)\n",
    "    test_acc = test(net, test_dataloader, device)\n",
    "    print(f\"TEST ACCURACY: {test_acc}\")\n",
    "\n",
    "    total_time = int(time.time() - start_time)\n",
    "    min = int(total_time / 60)\n",
    "    sec = total_time % 60\n",
    "    print(f\"\\nTotal time: {min} min {sec} sec\\n\")\n",
    "        \n",
    "    return train_losses, val_losses, val_accuracies, test_acc\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxYUli9d9uYQ",
    "colab_type": "text"
   },
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZcoQ5fD49yT_",
    "colab_type": "code",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "net = get_alexnet(pretrained=True)\n",
    "criterion, optimizer, scheduler = loss_optmizer_scheduler(net)\n",
    "\n",
    "train_losses_scratch, val_losses_scratch, \\\n",
    "accuracies_scratch, test_scratch_acc = train_model(net, criterion, optimizer, \n",
    "                                                   scheduler, file_path=BASE_FILE_PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ajilYpLrfNRR",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def save_results(file_name, train_losses, val_losses, accuracies):\n",
    "  with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"train_loss,val_loss,accuracy\\n\")\n",
    "    for tl,vl,accuracy in zip(train_losses, val_losses, accuracies):\n",
    "      f.write(f\"{tl},{vl},{accuracy}\\n\")\n",
    "\n",
    "save_results(f\"{BASE_FILE_PATH}_data.csv\", train_losses_scratch, val_losses_scratch, accuracies_scratch)\n",
    "print(\"*************** DATA SAVED *************\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ouyZnZRRryIr",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "epochs = np.arange(1, NUM_EPOCHS+1)\n",
    "plt.figure()\n",
    "# plt.scatter(epochs, train_losses, c=\"darkorange\", s=20)\n",
    "plt.plot(epochs, train_losses_scratch, label=\"train\")\n",
    "plt.plot(epochs, val_losses_scratch, label=\"val\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(epochs, accuracies_scratch, c=\"darkorange\", s=20)\n",
    "plt.plot(epochs, accuracies_scratch, zorder=-1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxekmR745ySe",
    "colab_type": "text"
   },
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fSHcUqLB5yWO",
    "colab_type": "code",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "with open(f\"{BASE_FILE_PATH}_final_result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "  f.write(\"*************** TRAINING FROM SCRATCH ***************\\n\")\n",
    "  f.write(f\"LR = {LR}\\nAccuracy on test = {test_scratch_acc}\\nLoss_min = {min(val_losses_scratch)}\\n\\n\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNm9RmvcbTcz",
    "colab_type": "text"
   },
   "source": [
    "**freezing layers**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uw5HyiIyLeCP",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def is_layers_unfreezed(model):\n",
    "  for param in model.parameters():\n",
    "    print(param.requires_grad)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CTWrTh9TbqVq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "net_conv_freezed = get_alexnet(True)\n",
    "# Freeze Conv Layers\n",
    "conv_layers = net_conv_freezed.features.parameters()\n",
    "\n",
    "for par in conv_layers:\n",
    "  par.requires_grad = False\n",
    "\n",
    "criterion, optimizer, scheduler = loss_optmizer_scheduler(net_conv_freezed)\n",
    "is_layers_unfreezed(net_conv_freezed)\n",
    "path = f\"{BASE_FILE_PATH}_freezing_conv\"\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dg8njYeh9s3W",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_losses_conv_freezed, val_losses_conv_freezed, \\\n",
    "accuracies_conv_freezed, test_acc_conv_freezed = train_model(net_conv_freezed, criterion, optimizer, \n",
    "                                                   scheduler, file_path=BASE_FILE_PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EWkwIELCmT1o",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "epochs = np.arange(1, NUM_EPOCHS+1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses_conv_freezed, label=\"train\")\n",
    "plt.plot(epochs, val_losses_conv_freezed, label=\"val\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(epochs, accuracies_conv_freezed, c=\"darkorange\", s=20)\n",
    "plt.plot(epochs, accuracies_conv_freezed, zorder=-1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z3G6A5zyrj8C",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "save_results(f\"{path}_data.csv\", train_losses_conv_freezed, val_losses_conv_freezed, accuracies_conv_freezed)\n",
    "\n",
    "with open(f\"{BASE_FILE_PATH}_final_result.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "  f.write(\"*************** TRAINING ONLY FULL ***************\\n\")\n",
    "  f.write(f\"LR = {LR}\\nAccuracy on test = {test_acc_conv_freezed}\\nLoss_min = {min(val_losses_conv_freezed)}\\n\\n\")\n",
    "\n",
    "print(\"*************** DATA SAVED *************\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5RMhhWWA3um",
    "colab_type": "text"
   },
   "source": [
    "**Freezing fc**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ERZDb-6uA2NE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "net_fc_freezed = get_alexnet(True)\n",
    "# Freeze Conv Layers\n",
    "fc = net_fc_freezed.classifier.parameters()\n",
    "\n",
    "for par in fc:\n",
    "  par.requires_grad = False\n",
    "\n",
    "criterion, optimizer, scheduler = loss_optmizer_scheduler(net_fc_freezed)\n",
    "is_layers_unfreezed(net_fc_freezed)\n",
    "path = f\"{BASE_FILE_PATH}_freezing_fc\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "mqabfU4UBrYF",
    "colab": {}
   },
   "source": [
    "train_losses_fc_freezed, val_losses_fc_freezed, \\\n",
    "accuracies_fc_freezed, test_acc_fc_freezed = train_model(net_fc_freezed, criterion, optimizer, \n",
    "                                                   scheduler, file_path=BASE_FILE_PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "v6ijsvLQCOce",
    "colab": {}
   },
   "source": [
    "epochs = np.arange(1, NUM_EPOCHS+1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses_fc_freezed, label=\"train\")\n",
    "plt.plot(epochs, val_losses_fc_freezed, label=\"val\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(epochs, accuracies_fc_freezed, c=\"darkorange\", s=20)\n",
    "plt.plot(epochs, accuracies_fc_freezed, zorder=-1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RsnrHftTCmJ5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "save_results(f\"{path}_data.csv\", train_losses_fc_freezed, val_losses_fc_freezed, accuracies_fc_freezed)\n",
    "\n",
    "with open(f\"{BASE_FILE_PATH}_final_result.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "  f.write(\"*************** TRAINING ONLY FULL ***************\\n\")\n",
    "  f.write(f\"LR = {LR}\\nAccuracy on test = {test_acc_fc_freezed}\\nLoss_min = {min(val_losses_fc_freezed)}\\n\\n\")\n",
    "\n",
    "print(\"*************** DATA SAVED *************\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA44mxi0ZmIO",
    "colab_type": "text"
   },
   "source": [
    "# Graph losses and accuracies over unfreezed and only_full nets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eH9Sg7hPZlE6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "epochs = np.arange(1, NUM_EPOCHS+1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, val_losses_scratch, label=\"unfreezed\")\n",
    "plt.plot(epochs, val_losses_conv_freezed, label=\"only linear\")\n",
    "plt.plot(epochs, val_losses_fc_freezed, label=\"only conv\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, accuracies_scratch, label=\"unfreezed\")\n",
    "plt.plot(epochs, accuracies_conv_freezed, label=\"conv freezed\")\n",
    "plt.plot(epochs, accuracies_fc_freezed, label=\"fc freezed\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}